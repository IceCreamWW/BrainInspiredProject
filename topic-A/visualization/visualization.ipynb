{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizable Illustration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we will plot the loss function of $J(\\theta_1, \\theta_2)$, which means our model consists of only two parameters\n",
    "\n",
    "We use `logistic regression` model with `MSE` loss function, which is in practice not a good setting, but what we need is it's `non-convex` property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets.samples_generator import make_classification\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 100\n",
    "lr = .5\n",
    "n_epochs = 500000\n",
    "print_every = n_epochs // 20\n",
    "patience = 1000\n",
    "debug=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss(Y, pred):\n",
    "    criterion1 = nn.MSELoss()\n",
    "    criterion2 = nn.L1Loss()\n",
    "    loss = criterion1(pred, Y) / criterion2(pred, Y)\n",
    "    # loss = criterion1(pred, Y)\n",
    "    # loss = criterion2(pred, Y.long())\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data generation\n",
    "generate classification data with only one feature ($f(x) = \\frac{1}{1 + e^{-w_1x_1 + w_2}}$) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data(n_samples):\n",
    "    x, y = make_classification(n_samples = n_samples,\n",
    "                   n_features=1, \n",
    "                   n_informative=1, \n",
    "                   n_redundant=0, \n",
    "                   n_repeated=0,\n",
    "                   n_clusters_per_class=1,\n",
    "                   random_state=777)\n",
    "\n",
    "\n",
    "    X = torch.from_numpy(x).double().flatten()\n",
    "    Y = torch.from_numpy(y).double().flatten()\n",
    "    \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X, Y):\n",
    "    no_improve_since = 0\n",
    "    min_loss = 1e5\n",
    "    \n",
    "    history = {\n",
    "        \"w1\": [],\n",
    "        \"w2\": [],\n",
    "        \"loss\": []\n",
    "    }\n",
    "    \n",
    "    torch.manual_seed(777)\n",
    "    theta = torch.tensor([40.0, 40], requires_grad=True)\n",
    "    # theta = torch.tensor([-12.0, 3], requires_grad=True)\n",
    "\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        output = torch.sigmoid(X * theta[0] + theta[1])\n",
    "        loss = get_loss(Y, output)\n",
    "        loss.backward()\n",
    "        \n",
    "        if loss < min_loss:\n",
    "            no_improve_since = 0\n",
    "            min_loss = loss\n",
    "        else:\n",
    "            no_improve_since += 1\n",
    "            if no_improve_since >= patience:\n",
    "                print(f\"reached patience {patience} after {epoch} epochs,  loss: {loss.item()}\")\n",
    "                break\n",
    "        \n",
    "        if loss < .25:\n",
    "            print(f\"converged after {epoch} epochs,  loss: {loss.item()}\")\n",
    "            break\n",
    "\n",
    "        if debug:\n",
    "            print(theta.grad.data)\n",
    "\n",
    "        # back propagation\n",
    "        theta.data = theta.data - theta.grad.data * lr\n",
    "        grad_to_print = theta.grad.data\n",
    "        theta.grad.data.zero_()\n",
    "\n",
    "        if not print_every == -1 and epoch % print_every == 1:\n",
    "            with torch.no_grad():\n",
    "                pred = (output > 0.5)\n",
    "                acc = torch.sum((pred == Y).float()) / Y.size()[0]\n",
    "            print(f\"epoch: {epoch - 1} \\t acc: {acc} \\t loss: {loss.item()} \\t theta: {theta.data}\")\n",
    "        \n",
    "        history['w1'].append(theta.detach().numpy()[0])\n",
    "        history['w2'].append(theta.detach().numpy()[1])\n",
    "        history['loss'].append(loss.item())\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        pred = (output > 0.5)\n",
    "        acc = torch.sum((pred == Y).float()) / Y.size()[0]\n",
    "    print(f\"converged after {epoch - 1} epochs \\t acc: {acc} \\t loss: {loss.item()} \\t theta: {theta.data}\")\n",
    "    \n",
    "    return X, Y, history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot and Save figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot3D_and_save(X, Y, history):\n",
    "    \n",
    "    w1range = np.linspace(-60 , 60, 60)\n",
    "    w2range = np.linspace(-60 , 60, 60)\n",
    "    W1, W2 = np.meshgrid(w1range, w2range)\n",
    "\n",
    "    J = np.array([get_loss(torch.sigmoid(theta[0] * X + theta[1]), Y).item()\n",
    "                   for theta in zip(torch.from_numpy(np.ravel(W1)), torch.from_numpy(np.ravel(W2)))])\n",
    "    J = J.reshape(W1.shape)\n",
    "\n",
    "\n",
    "    # draw and save figure\n",
    "    dirpath = f\"./figures/#samples/{n_samples}\"\n",
    "    os.makedirs(dirpath, exist_ok=True)\n",
    "    os.makedirs(\"./figures/compare (train)\", exist_ok=True)\n",
    "    os.makedirs(\"./figures/compare (contour)\", exist_ok=True)\n",
    "    os.makedirs(\"./figures/compare (surface)\", exist_ok=True)\n",
    "    os.makedirs(\"./figures/compare (surface + contour)\", exist_ok=True)\n",
    "    os.makedirs(\"./figures/compare (surface + train)\", exist_ok=True)\n",
    "    os.makedirs(\"./figures/compare (contour + train)\", exist_ok=True)\n",
    "    os.makedirs(\"./figures/compare (surface + contour + train)\", exist_ok=True)\n",
    "    \n",
    "\n",
    "    fig = plt.figure(figsize=(20, 10))\n",
    "    fig.suptitle(f\"Loss Function - {n_samples}\", fontsize=24, fontweight='bold')\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    ax.set_xlabel('w1', labelpad=30, fontsize=24, fontweight='bold')\n",
    "    ax.set_ylabel('w2', labelpad=30, fontsize=24, fontweight='bold')\n",
    "    ax.set_zlabel('J(w1,w2)', labelpad=30, fontsize=24, fontweight='bold')\n",
    "    # ax.view_init(elev=48., azim=42)\n",
    "    ax.view_init(elev=48., azim=75)\n",
    "    \n",
    "    surface = ax.plot_surface(W1, W2, J, alpha=0.9, cmap=cm.jet)\n",
    "    surface_alpha = ax.plot_surface(W1, W2, J, alpha=0.65, cmap=cm.jet)\n",
    "    train = ax.plot(history['w1'], history['w2'], history['loss'] , color='k', markerfacecolor='k', markeredgecolor='k', marker='.', markersize=5)[0]\n",
    "    train_z = ax.plot(history['w1'], history['w2'] , color='k', markerfacecolor='k', markeredgecolor='k', marker='.', markersize=1, zdir='z', alpha=0.7)[0]\n",
    "    train_y = ax.plot(history['w2'], history['loss'] , color='k', markerfacecolor='k', markeredgecolor='k', marker='.', markersize=1, zdir='y', zs=-65, alpha=0.7)[0]\n",
    "    train_x = ax.plot(history['w1'], history['loss'] , color='k', markerfacecolor='k', markeredgecolor='k', marker='.', markersize=1, zdir='x', zs=-65, alpha=0.7)[0]\n",
    "\n",
    "    \n",
    "    contourz = ax.contour(W1, W2, J, zdir='z', offset=0, cmap=cm.jet, alpha=1)\n",
    "    contoury = ax.contour(W1, W2, J, zdir='y', offset=-65, cmap=cm.jet, alpha=0.4)\n",
    "    contourx = ax.contour(W1, W2, J, zdir='x', offset=-65, cmap=cm.jet, alpha=0.4)\n",
    "    contours = [contourx, contoury, contourz]\n",
    "\n",
    "    def set_opacity(b1, b2, b3, b4):\n",
    "        surface.set_visible(b1)\n",
    "        surface_alpha.set_visible(b2)\n",
    "        train.set_visible(b3)\n",
    "        train_x.set_visible(b3)\n",
    "        train_y.set_visible(b3)\n",
    "        train_z.set_visible(b3)\n",
    "        list(map(lambda contourset: list(map(lambda contour: contour.set_visible(b4), contourset.collections)), contours))\n",
    "\n",
    "\n",
    "    # surface\n",
    "    set_opacity(True, False, False, False)\n",
    "    \n",
    "    plt.savefig(os.path.join(dirpath, 'surface.png'), transparent=True)\n",
    "    plt.savefig(f'./figures/compare (surface)/{n_samples}.png', transparent=True)\n",
    "    \n",
    "    \n",
    "    # contour\n",
    "    set_opacity(False, False, False, True)\n",
    "    \n",
    "    plt.savefig(os.path.join(dirpath, 'contour.png'), transparent=True)\n",
    "    plt.savefig(f'./figures/compare (contour)/{n_samples}.png', transparent=True)\n",
    "    \n",
    "    \n",
    "    # train\n",
    "    set_opacity(False, False, True, False)\n",
    "    \n",
    "    plt.savefig(os.path.join(dirpath, 'train.png'), transparent=True)\n",
    "    plt.savefig(f'./figures/compare (train)/{n_samples}.png', transparent=True)\n",
    "    \n",
    "    # surface + contour\n",
    "    set_opacity(False, True, False, True)\n",
    "    \n",
    "    plt.savefig(os.path.join(dirpath, 'surface + contour.png'), transparent=True)\n",
    "    plt.savefig(f'./figures/compare (surface + contour)/{n_samples}.png', transparent=True)\n",
    "    \n",
    "    # surface + train\n",
    "    set_opacity(False, True, True, False)\n",
    "    \n",
    "    plt.savefig(os.path.join(dirpath, 'surface + train.png'), transparent=True)\n",
    "    plt.savefig(f'./figures/compare (surface + train)/{n_samples}.png', transparent=True)\n",
    "    \n",
    "    # contour + train\n",
    "    set_opacity(False, False, True, True)\n",
    "    \n",
    "    plt.savefig(os.path.join(dirpath, 'contour + train.png'), transparent=True)\n",
    "    plt.savefig(f'./figures/compare (contour + train)/{n_samples}.png', transparent=True)\n",
    "    \n",
    "    # surface + contour + train\n",
    "    set_opacity(False, True, True, True)\n",
    "    \n",
    "    plt.savefig(os.path.join(dirpath, 'surface + contour + train.png'), transparent=True)\n",
    "    plt.savefig(f'./figures/compare (surface + contour + train)/{n_samples}.png', transparent=True)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot2D_and_save(X, Y, history):\n",
    "    # draw and save figure\n",
    "    dirpath = f\"./figures/#samples/{n_samples}\"\n",
    "    os.makedirs(dirpath, exist_ok=True)\n",
    "    os.makedirs(\"./figures/compare (contourz + train)\", exist_ok=True)\n",
    "    os.makedirs(\"./figures/compare (contourz)\", exist_ok=True)\n",
    "    \n",
    "    \n",
    "    w1range = np.linspace(-60 , 60, 60)\n",
    "    w2range = np.linspace(-60 , 60, 60)\n",
    "    W1, W2 = np.meshgrid(w1range, w2range)\n",
    "\n",
    "    J = np.array([get_loss(torch.sigmoid(theta[0] * X + theta[1]), Y).item()\n",
    "                   for theta in zip(torch.from_numpy(np.ravel(W1)), torch.from_numpy(np.ravel(W2)))])\n",
    "    J = J.reshape(W1.shape)\n",
    "    \n",
    "    plt.clf()\n",
    "    fig = plt.figure(figsize=(20, 10))\n",
    "    fig.suptitle(f\"Loss Function - {n_samples}\", fontsize=24, fontweight='bold')\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.set_xlabel('w1', labelpad=30, fontsize=24, fontweight='bold')\n",
    "    ax.set_ylabel('w2', labelpad=30, fontsize=24, fontweight='bold')\n",
    "\n",
    "    contourz = ax.contour(W1, W2, J, cmap=cm.jet)\n",
    "    plt.savefig(os.path.join(dirpath, 'contourz.png'), transparent=True)\n",
    "    plt.savefig(f'./figures/compare (contourz)/{n_samples}.png', transparent=True)\n",
    "    \n",
    "    train_z = ax.plot(history['w1'], history['w2'] , color='k', markerfacecolor='k', markeredgecolor='k', marker='.', markersize=1)[0]\n",
    "    plt.savefig(os.path.join(dirpath, 'contourz + train.png'), transparent=True)\n",
    "    plt.savefig(f'./figures/compare (contourz + train)/{n_samples}.png', transparent=True)\n",
    "    \n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save result and data for further processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_csv(X, Y, history):\n",
    "    dirpath = f'./result/{n_samples}'\n",
    "    os.makedirs(dirpath, exist_ok=True)\n",
    "    pd.DataFrame.from_dict(history).to_csv(os.path.join(dirpath, 'history.csv'))\n",
    "    \n",
    "    data = np.c_[X.detach().numpy(), Y.detach().numpy()]\n",
    "    np.savetxt(os.path.join(dirpath, 'data.csv'), data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_from_csv(n):\n",
    "    history = pd.read_csv(f\"./result/{n}/history.csv\").reset_index().to_dict(orient='list')\n",
    "    data = np.loadtxt(f\"./result/{n}/data.csv\")\n",
    "    X = torch.from_numpy(data[:, 0])\n",
    "    Y = torch.from_numpy(data[:, 1])\n",
    "    return X, Y, history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: MacOSX\n",
      "epoch: 0 \t acc: 0.7432680130004883 \t loss: 0.9790044882843658 \t theta: tensor([39.9999, 39.9996])\n",
      "epoch: 25000 \t acc: 0.7648209929466248 \t loss: 0.9758872108886384 \t theta: tensor([38.5499, 33.9701])\n",
      "epoch: 50000 \t acc: 0.7911890149116516 \t loss: 0.9702877236293741 \t theta: tensor([35.9202, 26.1194])\n",
      "epoch: 75000 \t acc: 0.8239780068397522 \t loss: 0.9561894149388893 \t theta: tensor([29.4430, 14.8813])\n",
      "reached patience 1000 after 92247 epochs,  loss: 0.3976490785195139\n",
      "converged after 92246 epochs \t acc: 0.8591330051422119 \t loss: 0.3976490785195139 \t theta: tensor([ 0.8587, -0.0198])\n"
     ]
    }
   ],
   "source": [
    "%matplotlib\n",
    "# for n_samples in [100000, 10000, 1000, 100, 50, 20, 10, 6, 4]:\n",
    "for n_samples in [1000000]:\n",
    "    X, Y = make_data(n_samples)\n",
    "    X, Y, history = train(X, Y)\n",
    "    # save_to_csv(X, Y, history)\n",
    "    plot2D_and_save(X, Y, history)\n",
    "    plot3D_and_save(X, Y, history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: MacOSX\n"
     ]
    }
   ],
   "source": [
    "%matplotlib\n",
    "for n_samples in [100000, 10000, 1000, 100, 50, 20, 10]:\n",
    "# for n_samples in [10000]:\n",
    "    X, Y, history = load_from_csv(n_samples)\n",
    "    # plot3D_and_save(X, Y, history)    \n",
    "    plot2D_and_save(X, Y, history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n%matplotlib\\nn_samples = 100000\\ntrain_args = make_data()\\nplot_args = train(*train_args)\\nplot3D_and_save(*plot_args)\\n'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "%matplotlib\n",
    "n_samples = 100000\n",
    "train_args = make_data()\n",
    "plot_args = train(*train_args)\n",
    "plot3D_and_save(*plot_args)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Blank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfig = plt.figure(figsize=(20, 10))\\nfig.suptitle(f\"Loss Function Smoothness\", fontsize=24, fontweight=\\'bold\\')\\nax = fig.add_subplot(111, projection=\\'3d\\')\\nax.set_xlabel(\\'w1\\', labelpad=30, fontsize=24, fontweight=\\'bold\\')\\nax.set_ylabel(\\'w2\\', labelpad=30, fontsize=24, fontweight=\\'bold\\')\\nax.set_zlabel(\\'J(w1,w2)\\', labelpad=30, fontsize=24, fontweight=\\'bold\\')\\nax.view_init(elev=48., azim=42)\\nplt.show()\\n'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "fig = plt.figure(figsize=(20, 10))\n",
    "fig.suptitle(f\"Loss Function Smoothness\", fontsize=24, fontweight='bold')\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.set_xlabel('w1', labelpad=30, fontsize=24, fontweight='bold')\n",
    "ax.set_ylabel('w2', labelpad=30, fontsize=24, fontweight='bold')\n",
    "ax.set_zlabel('J(w1,w2)', labelpad=30, fontsize=24, fontweight='bold')\n",
    "ax.view_init(elev=48., azim=42)\n",
    "plt.show()\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
